# Cache4AI
# 语义缓存

语义缓存通过理解查询的语义意义来提升缓存效率，特别适用于LLM应用。

- **GPTCache**：GitHub上的项目展示了其与LangChain和llama_index的集成
    - 链接：[GPTCache GitHub](https://github.com/zilliztech/GPTCache)
    - A Library for Creating Semantic Cache for LLM Queries，Slash Your LLM API Costs by 10x 💰, Boost Speed by 100x ⚡
    - 值得注意的是，Milvus向量数据库也是他们做的。
    - ![[Cache Layer for LLM.png]]
- **Redis的语义缓存**：
    - 链接：[Redis Blog - What is Semantic Caching?](https://redis.io/blog/what-is-semantic-caching/)
    - Redis的SemanticCache文档：2025年的文档详细说明了RedisVL如何通过向量搜索实现语义缓存，减少LLM请求。
        - 链接：[Redis Documentation - SemanticCache](https://redis.io/docs/latest/integrate/redisvl/user_guide/llmcache/)
	- Azure Managed Redis：2025年5月20日的Microsoft Learn教程展示了如何使用Azure Managed Redis进行语义缓存，结合Azure OpenAI服务提升响应速度。链接：[Microsoft Learn - Azure Managed Redis as Semantic Cache](https://learn.microsoft.com/en-us/azure/redis/tutorial-semantic-cache)
	- LangCache by Redis：2025年4月8日推出的Redis管理式语义缓存服务，优化GenAI应用和RAG管道，承诺减少90%的LLM调用。链接：[LangCache by Redis](https://redis.io/langcache/)
- **Leveraging Approximate Caching for Faster Retrieval-Augmented Generation**
	- [https://doi.org/10.1145/3721146.3721941](https://doi.org/10.1145/3721146.3721941)
	- 会议：2025年4月1日，5th Workshop on Machine Learning and Systems (co-located with ACM SIGMOD/PODS 2025)
	- 关键贡献：
		提出近似缓存技术，通过重用相似的提示来加速检索增强生成（RAG），降低LLM推理的延迟。
		在实际工作负载上实现了显著的性能提升，适合需要快速响应的AI应用。
		相关性：该论文虽然主要针对RAG，但其近似缓存技术与AI4Cache高度相关，发表在与SIGMOD相关的顶级工作坊。

### 问题：
1. 目前只有两个工程项目比较火，redis-SemanticCache（2024-7），GPTCache（2025），<span style="background:#fff88f">缺少论文支撑</span>
2. Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data（好像中了IPDPS）
3. MeanCache: User-Centric Semantic Cache for Large Language Model Based Web Services
4. SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models（这两个文章都是对比GPTCache的，都在arxiv上）



# AI4Cache：
1. 3L-Cache: Low Overhead and Precise Learningbased Eviction Policy for Caches
	1. https://www.usenix.org/system/files/fast25-zhou-wenbin.pdf
	2. 开源：https://github.com/optiq-lab/3L-Cache
2.  iCache: An Intelligent Cache Allocation Strategy for Multitenant in High-Performance Solid-State Disks
3. Advancements in cache management: a review of machine learning innovations for enhanced performance and security
	    - 全面综述机器学习在缓存管理中的应用，涵盖三个主要领域：
	        - **缓存替换**：讨论了从传统启发式方法到机器学习的转变，介绍了RLR（单核和四核系统性能比LRU分别提高3.25%和4.86%）、Glider（单核系统未命中率比LRU降低8.9%）、LeCaR（小缓存尺寸下性能比ARC提高18倍）、CACHEUS、PARROT和Seq2Seq（比LRU、LFU和ARC分别提高77%、65%和77%）。
	        - **边缘网络内容缓存**：包括DeepCache（使用深度LSTM预测内容流行度）、PA-Cache（使用多层RNN降低计算成本）、RL-Cache（在Akamai数据上提升缓存命中率）。
	        - **缓存安全**：讨论了机器学习在检测缓存侧信道攻击（如Flush + Reload、Flush + Flush、Specter）中的应用，模型准确率高达99.92%（Tong et al., 2020）。
	    - 分析了机器学习在硬件、边缘设备和大规模云系统中的应用，强调其适应工作负载特性和提高缓存命中率的能力。
	- **相关性**：作为2025年的综述文章，该论文涵盖了AI4Cache的最新进展，引用了大量2024和2025年的研究，适合作为研究起点。
	- **链接**：链接：[Frontiers in Artificial Intelligence](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1441250/full)
4.  https://arxiv.org/html/2501.14770v1
	1. 该研究比较了机器学习方法与传统缓存策略（如写穿、写回和 LRU 逐出）的性能，显示机器学习方法在所有工作负载下均取得了最高的缓存命中率和最长的 SSD 寿命。以下是关键性能指标的示例：

# 问题
1. 可以参考现有方法，做面向SSD的AI Cache策略
2. 性能问题，现有文章的实验多数都是在模拟。
3. 机器学习方法本身不好提升，这个我不擅长。
4. FDP：擅长的场景，降低写放大；降低延迟；在容量满时维持性能稳定。
