#Cache #FDP
[[Towards Efficient Flash Caches with Emerging NVMe.pdf]]
[[Paper_in_md/Towards Efficient Flash Caches with Emerging NVMe.pdf-bde1b412-5210-41ef-8477-aaf2a7d18544/full|full]]


> Cachelib适配FDP

# 利用新兴 NVMe 灵活数据放置 SSD 实现高效闪存缓存

## 摘要

NVMe 闪存 SSD 已广泛部署在数据中心，用于缓存大规模 Web 服务的工作集。随着数据中心面临日益增长的可持续性需求，如减少碳排放，高效管理闪存过度配置和耐久性变得至关重要。我们的分析表明，在闪存块上混合不同生命周期的数据会导致高昂的设备垃圾回收成本，这要么减少设备寿命，要么需要主机过度配置。在闪存上进行有针对性的数据放置以最小化数据混合，从而减少设备写入放大，显示出解决此问题的希望。

NVMe 灵活数据放置(FDP)提案是一项新批准的技术提案，旨在解决数据放置需求，同时减少与过去存储接口(如 ZNS 和开放通道 SSD)相关的软件工程成本。在本研究中，我们探讨了在 CacheLib 中利用 NVMe FDP 原语进行闪存媒体数据放置的可行性、优势和局限性。CacheLib 是一个流行的开源闪存缓存，在 Meta 的软件生态系统中被广泛部署和使用作为缓存构建块。我们证明，使用 NVMe FDP SSD 在 CacheLib 中进行有针对性的数据放置有助于减少设备写入放大、内含碳排放和功耗，几乎不会对其他指标产生影响。使用来自 Meta 和 Twitter 的多个生产跟踪及其配置，我们表明使用 FDP 可以实现理想的设备写入放大约为 1，从而提高 SSD 利用率并实现可持续的闪存缓存部署。

## 1 引言

缓存是现代大规模 Web 服务架构中用于高性能和更好资源利用的直观且普遍的技术。这些 Web 服务涵盖各种领域，例如社交网络、微博平台和新兴的物联网应用等。该领域缓存解决方案设计的挑战是(1)大型工作集大小和(2)缓存不同大小对象的问题，特别是由众多小型对象主导的问题。与 DRAM 和 HDD 相比，基于闪存的 SSD 在这些缓存的设计中变得流行，因为它们具有出色的性能成本权衡。然而，在这些缓存中管理闪存的有限写入耐久性仍然是一个挑战。为了最大化闪存缓存中的闪存寿命，已经对准入策略、应用程序写入放大和缓存算法进行了广泛研究。然而，闪存缓存中设备级写入放大(DLWA)的问题尚未得到足够关注。

设备级写入放大问题，节能问题。

最先进的闪存缓存采用专门的引擎分别来缓存小型和大型对象。使用**集合关联缓存**设计来最小化跟踪众多小对象的开销，而使用**日志结构**设计来缓存大对象以生成闪存友好的写入。这两种缓存设计在 SSD 上有不同的写入模式。集合关联缓存以随机方式产生频繁更新，而日志结构缓存以顺序方式在 SSD 上产生不频繁的更新。为了抵消高 DLWA，这些缓存的生产部署会降低闪存设备的利用率，导致大量内含碳足迹。CacheLib 的生产部署(这是一个开源闪存缓存，用作在 Meta 构建和部署 100 多个服务的缓存构建块)仅利用了闪存容量的 50%。我们的分析表明，这些缓存设计中高 DLWA 的原因是来自两个不同缓存引擎的数据混合。在闪存上进行有针对性的数据放置以隔离来自专门引擎的数据有望减少 DLWA 和内含碳足迹。

![CacheLib架构概述](d:\ReadingPaper\Paper_in_md\Towards Efficient Flash Caches with Emerging NVMe.pdf-bde1b412-5210-41ef-8477-aaf2a7d18544\images\e91060924d73fb994f34e19bfcdeb7b3edbf77622059927a652f9f8c09d3cdc7.jpg)

图 1. CacheLib 架构概述

## 2 背景

### 2.1 SSD 和写入放大

**SSD 基础知识**：SSD NAND 封装组织为裸片、平面、块和页。由于擦除后写入的特性，NAND SSD 不能直接覆写。擦除操作以擦除块(EB)(数十到数百 MB)为单位进行，而写入以页(16KB、48KB、64KB 等)为单位进行。SSD 中的闪存转换层(FTL)通过以下方式处理覆写：(1)在空闲页中写入(编程)新数据；(2)使旧数据无效；(3)更新元数据以指向新数据。元数据将逻辑地址转换为 NAND 媒体中的物理地址。向 SSD 中的逻辑地址写入会创建无效页，这些页必须通过称为垃圾回收(GC)的过程回收。

**SSD 垃圾回收(GC)**：当 SSD 中空闲块不足时，会触发垃圾回收过程。此过程从擦除块中读取剩余有效页并将它们编程到新位置。现在完全无效的擦除块在空闲池中可用。空闲池中的任何擦除块都可能被擦除并用下一个传入的数据进行编程。垃圾回收是一项昂贵的操作，SSD 消耗的能量与垃圾回收操作的数量和持续时间成正比。

**设备级写入放大(DLWA)**：DLWA 是一个指标，用于量化 SSD 内部写入的数据量与主机实际发送到 SSD 的数据量之比。计算公式如下：

$$
\mathrm{DLWA} = \frac{\mathrm{Total~NAND~Writes}}{\mathrm{Total~SSD~Writes}} \tag{1}
$$

**应用级写入放大(ALWA)**：ALWA 是一个指标，用于量化发送到 SSD 写入的数据量与应用程序实际接收到要写入的数据量之比。计算公式如下：

$$
\mathrm{ALWA} = \frac{\mathrm{Total~SSD~Writes}}{\mathrm{Total~Application~Writes}} \tag{2}
$$

**DLWA 的重要性**：垃圾回收产生的额外读写会干扰 SSD 中其他命令的处理，影响 QoS。此外，额外的 NAND 活动将消耗 NAND 媒体的有限耐久性。DLWA 为 2 意味着用户每写入 4KB 数据，FTL 由于垃圾回收额外写入了 4KB。由于 NAND 媒体具有固定数量的编程和擦除周期(P/E 周期)，之后它只能读取或变得有故障，DLWA 为 2 会导致设备寿命减半。设备级写入放大会影响其他 SSD 性能指标，如 QoS、带宽、寿命、可靠性和功耗。它通常用作监控 SSD 性能的简单代理指标。

### 2.2 DLWA 和碳排放

SSD 的寿命与设备级写入放大成反比。DLWA 为 2 会导致 SSD 故障速度是 DLWA 为 1 时的两倍。高 DLWA 会导致 SSD 过早故障，需要频繁更换设备。SSD 制造每年产生数百万公吨的 CO2 排放。这些排放广泛归类为内含碳排放。随着系统从 HDD 转向 SSD，减少 DLWA 变得非常重要，因为 SSD 的内含碳成本至少比 HDD 高一个数量级。减少 DLWA 可以在大规模上摊销基于闪存系统的资本成本和内含碳排放。

高 DLWA 是由于增加了垃圾回收操作，以移动有效页面来释放 SSD 块。因此，SSD 在活动状态下花费的时间比在空闲状态下多，这导致更大的能源消耗。较低的 DLWA 导致较低的运行能源消耗，转化为更高的运行碳效率。尽管运行碳效率优化很重要，但预计不会从中获得大的碳效率收益。这是因为 SSD 设计为节能，并优化为在不使用时切换到空闲状态。

### 2.3 闪存缓存和 CacheLib

**闪存中的缓存**：缓存在大规模 Web 服务中广泛使用，以提供高性能并降低运营成本。由于与 DRAM 和 HDD 相比具有出色的价格性能权衡，基于闪存的 SSD 已成为缓存流行 Web 服务的大型工作集的流行选择。闪存上的缓存是写入密集型的，因为从 DRAM 读取时的驱逐转化为闪存上的写入。随着工作集大小的增加、键的变化和部署中 DRAM 大小的减少，对闪存缓存的写入会增加。然而，闪存的有限写入耐久性加上不可预测的工作负载，对这些缓存的设计构成了挑战。

最近的研究已经调查了缓存算法、准入策略和应用程序级写入放大的设计，以管理闪存的有限设备耐久性，同时提供高命中率，但设备级写入放大(DLWA)在这一领域是一个研究不足的问题。随着可持续性挑战的增加，与 DRAM 相比，基于闪存的 SSD 将变得越来越有吸引力，以在可接受的性能下缓存大型工作集大小。然而，在声称基于闪存的 SSD 是可持续性的灵丹妙药之前，需要研究闪存缓存的 DLWA。

**CacheLib 架构**：CacheLib 是一个开源缓存库，作为 Meta 中 100 多个服务的基本缓存构建块被广泛使用和部署。它采用混合缓存架构(见图 1)，利用 DRAM 和基于闪存的 SSD 分层缓存数据。DRAM 用于缓存最受欢迎的项目，而 SSD 缓存从 DRAM 缓存中驱逐的不太受欢迎的数据。SSD 缓存进一步分解为小对象缓存(SOC)和大对象缓存(LOC)，以存储不同大小的对象。在部署时，可以配置将对象分类为小或大的阈值，以及 DRAM 和 SSD 的大小。CacheLib 的单个实例可以包含多个 DRAM 和 SSD 缓存引擎，每个引擎都有其配置的资源预算。

SOC 采用集合关联缓存设计，以桶(通常与 4KB 页面大小对齐)为单位执行就地 SSD 写入，并利用统一哈希函数来最小化跟踪众多小对象的开销。LOC 采用日志结构设计，以大区域(16 MB、256 MB 等)为单位执行 SSD 友好的写入，这些区域与擦除块大小对齐。LOC 可以配置为使用 FIFO 或 LRU 驱逐策略。SOC 和 LOC 的优缺点相互补充。LOC 具有 SSD 友好的写入模式，但跟踪对象有 DRAM 开销，而 SOC 具有 SSD 不友好的写入模式，几乎没有跟踪对象的开销。

**生产闪存缓存和 CacheLib 的挑战**：野外部署的闪存缓存的核心挑战是管理闪存的有限耐久性，同时确保高命中率和低索引开销。它们必须处理具有不同大小和访问模式的对象的混合工作负载。大型缓存服务通常处理数十亿个频繁访问的小项目和数百万个不频繁访问的大项目。使用主机过度配置和阈值准入策略是减少 DLWA 的常见做法。在生产 CacheLib 部署中，50%的闪存容量被过度配置，以将 DLWA 保持在约 1.3 的可接受水平。在未来，增加闪存利用率并保持低 DLWA 对于大规模可持续部署闪存缓存至关重要。

## 3 NVMe 灵活数据放置(FDP)

### 3.1 概述

批准的 NVMe 灵活数据放置技术提案代表了 SSD 数据放置领域的演进，基于过去十年在野外学到的经验教训。它是谷歌的 SmartPTL 和 Meta 的直接放置模式提案的合并，旨在在闪存媒体上启用数据放置，而不需要 ZNS 的显式垃圾回收和开放通道 SSD 提案的低级媒体控制的高软件工程成本。它借鉴了十年前提出的多流 SSD 接口的元素，但由于缺乏行业和学术界的兴趣而没有真正起飞。它的设计考虑了向后兼容性，因此应用程序可以不变地与之配合使用。利用数据放置并评估其成本和收益的选择留给了应用程序。这使得工程努力的投入可以按需付费，而不是前期成本。

![传统SSD与FDP SSD架构](d:\ReadingPaper\Paper_in_md\Towards Efficient Flash Caches with Emerging NVMe.pdf-bde1b412-5210-41ef-8477-aaf2a7d18544\images\4d3975deee692eb48cb498b2e86ec70b729f9181c7dcfbe89a3998c84ebfacca.jpg)

图 2. 传统 SSD 与 FDP SSD 架构

### 3.2 使用 FDP 在 SSD 中进行物理隔离

#### 3.2.1 FDP 架构概念

灵活数据放置接口为主机提供抽象，以在设备上对具有类似预期寿命(例如，死亡时间)的数据进行分组。该接口引入了以下概念来公开 SSD 物理架构(见图 2)：

**回收单元(RU)**：NAND 媒体组织成一组回收单元，其中一个回收单元由一组可以写入的块组成。回收单元通常由一个或多个擦除块组成，但在提案中没有对此做出保证。RU 的大小由 SSD 制造商决定。在本文中，我们的设备具有超级块大小的 RU，其中超级块是 SSD 中跨裸片平面的擦除块的集合。如果 SSD 有 8 个裸片，每个裸片有 2 个平面，每个平面有 2 个擦除块，则超级块将由 32 个擦除块组成。

**回收组(RG)**：回收组是一组回收单元。

**回收单元句柄(RUH)**：回收单元句柄是设备控制器中类似于指针的抽象，允许主机软件指向设备中的回收单元。由于主机不能直接寻址回收单元，主机软件使用回收单元句柄来逻辑隔离数据。设备管理回收单元句柄到回收单元的映射，并对此映射有完全控制权。设备中 RUH 的数量决定了主机软件可以同时放置数据的 NAND 中不同逻辑位置的数量。

**RUH 类型**：FDP 接口指定了两种类型的回收单元句柄，每种类型在垃圾回收期间提供不同的数据移动保证，以及它们各自的权衡。在垃圾回收期间，RUH 类型用于确定要移动的数据的源和目标 RU。FDP 定义了两种 RUH 类型，即：

1. **初始隔离**：由此类型的 RUH 指向的回收组内的所有回收单元都是数据移动的候选者。对于多个初始隔离类型的 RUH，使用一个初始隔离类型的 RUH 写入的数据最初与使用另一个初始隔离类型的 RUH 写入的数据隔离。然而，在垃圾回收时，使用这两个句柄写入的有效数据可能会混合。这种类型在 SSD 控制器上实现成本最低，因为它不需要显式跟踪使用 RUH 写入的数据，并且对垃圾回收期间的数据移动施加的约束最少。

2. **持久隔离**：回收组内所有使用 RUH 写入的回收单元是垃圾回收时数据移动的唯一候选者。这种 RUH 类型提供了更强的数据隔离保证，但在控制器上实现成本较高，因为它需要显式跟踪使用 RUH 写入的数据，并对垃圾回收期间的数据移动施加更多约束。

**示例**：考虑使用两个 RUH(RUH0 和 RUH1)的写入模式，其中 RUH0 已将 LBA 写入 RU0 和 RU1，而 RUH1 已将 LBA 写入 RU2。为简单起见，假设所有 RU 都属于同一回收组。如果 RUH0 和 RUH1 是初始隔离类型，则在垃圾回收时，来自 RU0、RU1 和 RU2 的有效数据是移动的候选者，并且可以混合。如果 RUH0 和 RUH1 是持久隔离类型，则在垃圾回收时，只有来自 RU0 和 RU1 的数据可以混合，而 RU2 中的数据与 RU0 和 RU1 中的数据隔离。

**FDP 配置**：FDP 配置定义了 RUH、RUH 类型(初始或持久隔离)、它们与 RG 的关联以及 RU 大小。设备上可用的 FDP 配置由制造商预先确定，不能更改。本文使用具有单个 FDP 配置的 SSD，该配置具有 8 个初始隔离 RUH、1 个 RG 和 6GB 的 RU 大小。设备可以支持多种配置，由主机选择。

#### 3.2.2 使用 RUH 进行数据放置

在本节中，我们强调 FDP 接口的重要方面，这些方面影响主机的数据放置设计。

**使用 RUH 物理隔离逻辑块**：FDP 存储接口不引入任何新的命令集来写入设备。相反，定义了一个新的数据放置指令，允许每个写入命令指定一个 RUH。因此，主机软件可以使用 RUH 将逻辑块放置在利用 RUH 的 RU 中。通过允许主机动态地将逻辑块与 RU 关联，FDP 能够基于不同的温度和死亡时间(例如，热数据和冷数据分离)或不同的数据流(例如，大流和小日志)灵活地对数据进行分组。这有助于以物理隔离的方式写入不同的 RU。通过仔细地取消分配先前写入的 RU 中的所有数据，主机可以实现约为 1 的 DLWA。

在命名空间创建期间，主机软件选择由创建的命名空间访问的 RUH 列表。由于 FDP 向后兼容，如果未指定，设备会为命名空间选择默认 RUH。在没有来自主机的放置指令的情况下，数据放置在此 RUH 中。读取操作保持不变。FDP 中的写入可以跨越 RU 边界。如果写入操作因 RU 写满其容量而溢出 RU，设备会选择一个新的 RU 并更新 RUH 到新 RU 的映射。尽管此过程对主机不可见，但 SSD 会在设备日志中记录该事件，主机可以检查这些日志。

**管理无效化和跟踪 RU**：由于 FDP 不关注垃圾回收而仅关注数据放置，它不引入任何用于擦除操作的新抽象。与传统 SSD 一样，LBA 通过两种方式无效或取消分配：(1)通过覆写 LBA，(2)通过在一个或多个 LBA 上显式使用修剪操作。如果 RU 中的所有数据都无效，则 RU 被擦除以供将来写入，并且在垃圾回收时不必跨 RU 复制逻辑块。由于主机软件只能访问 RUH 而不能访问 RU，为了执行细粒度和有针对性的 RU 取消分配，主机软件需要跟踪已一起写入 RU 的 LBA 并取消分配这些 LBA。FDP 规范还允许主机查询当前由 RUH 引用的 RU 中的可用空间。

### 3.3 FDP 事件和统计信息

FDP 提供了一组详细的事件和垃圾回收统计信息，供主机跟踪 SSD 中与 FDP 相关的事件。这些有助于主机了解设备级异常，并确保主机和设备在数据放置方面保持同步。

### 3.4 FDP 与其他主要数据放置提案

NVMe FDP 技术提案是基于与过去数据放置提案集成软件堆栈的经验教训而构思的。它的设计专注于数据放置，允许主机软件堆栈执行数据隔离，同时将 NAND 媒体管理和垃圾回收留给 SSD 控制器。在表 1 中，我们概述了过去几年主要数据放置提案之间的一些关键差异。

### 3.5 局限性

1. **新兴和不断发展的技术**：FDP 技术提案于 2022 年底获得批准，一些来自三星的设备，如 PM9D3a，正在市场上出现并支持它，同时还有其他供应商的产品。由于批准相对较近，该提案可能会随着时间的推移进行修改，以包括所需功能的扩展。

2. **缺乏主机对垃圾回收的控制**：FDP 专门设计用于数据放置，同时允许主机对 LBA 进行随机写入，使 SSD 能够管理垃圾回收。因此，除了通过取消分配或覆写 LBA 来使 LBA 无效外，主机对 SSD 中的垃圾回收过程没有控制权。请注意，此限制仅适用于主机可以通过比 SSD 更有效地管理垃圾回收而获得更大性能收益的情况，而仅仅是专注于数据放置。

3. **需要设备过度配置和 SSD 中的映射表**：与当今的传统 SSD 一样，FDP SSD 也需要 DRAM 中的映射表来支持逻辑到物理地址的透明映射。此外，在没有基于主机的垃圾回收的情况下，设备中需要 NAND 过度配置以获得可接受的性能。当从 FDP SSD 制造成本的角度来看该提案时，这是一个限制。

## 4 为什么 FDP 对 CacheLib 和混合缓存很重要？

在本节中，我们基于对 CacheLib 的闪存缓存架构、Web 服务缓存部署和工作负载的分析，讨论 FDP 的适用性及其为 CacheLib 和混合缓存提供的机会。

### 4.1 见解和观察

**见解 1：SOC 和 LOC 数据的混合导致高 DLWA**。大型缓存项目以日志结构方式写入 LOC，利用 FIFO 或 LRU 驱逐策略。这导致对 SSD 的顺序写入模式。小型缓存项目使用统一哈希函数写入 SOC 桶。每次项目插入都会导致整个 SOC 桶(大小可配置，但默认为 4 KB)写入 SSD。与 LOC 相反，SOC 写入生成对 SSD 的随机写入模式。

对于具有大型工作集大小和键变化的工作负载，闪存缓存层由于从 RAM 缓存中驱逐而接收写入。对于小对象访问占主导地位的工作负载，这种隔离导致 LOC 中不频繁和冷数据访问模式与 SOC 中频繁和热数据访问模式并存。这导致 LOC 的顺序和冷数据与 SOC 的随机和热数据在单个 SSD 块中混合(图 3a 1a)，在垃圾回收时导致高 DLWA。

![SSD横截面](d:\ReadingPaper\Paper_in_md\Towards Efficient Flash Caches with Emerging NVMe.pdf-bde1b412-5210-41ef-8477-aaf2a7d18544\images\f36ee9c50f7286d326f6e41f60c8d838d4b7e2302d41d1225771d47494e921d5.jpg)

图 3. SSD 横截面。1a 显示了 LOC 的顺序和冷数据与 SOC 的随机和热数据在 SSD 块中的混合。1b 显示了 LOC 和 SOC 数据对设备 OP 的低效使用。2a 显示了 SOC 数据被隔离后，其数据的无效化可以导致空闲 SSD 块。2b 显示了使用 FDP，顺序写入的 LOC 数据不会导致 DLWA。2c 显示了 SOC 数据对设备 OP 的高效独占使用，以缓解 SOC DLWA。

**见解 2：使用主机过度配置作为 DLWA 的控制措施效率低下**。如第 2.3 节所述，CacheLib 部署利用 SSD 的近 50%的主机过度配置，将 DLWA 限制在约 1.3 的可接受值。从成本和碳效率的角度来看，这是低效的。由于其顺序和冷访问模式，LOC 数据不需要任何主机或设备过度配置即可实现 DLWA 为 1。没有主机过度配置，唯一可用于帮助控制 DLWA 的额外空间是设备过度配置空间。随机 SOC 数据将从设备过度配置空间中受益最多，因为它小、热且频繁更新。然而，SOC 和 LOC 的混合导致设备过度配置空间的低效使用(图 3a 1b)，因为 SOC 和 LOC 数据共享它，导致不必要的数据移动。

**见解 3：高 SOC 无效化和其小尺寸可以被利用来控制 DLWA**。设备上较小的 SOC 大小导致较少的桶和较高的键冲突率。由于整个 4KB 的 SOC 桶被写出，较大的 SOC 桶无效率对 SSD 友好，因为它导致更多的 SSD 页面无效。如果 SSD 擦除块中只有无效的 SOC 数据，这将导致整个擦除块释放自己，不需要移动有效数据。对于小对象访问占主导地位的工作负载，SOC 发生高无效化，但擦除块中的 SOC 数据与 LOC 数据混合。这阻止了 SSD 利用在小 LBA 空间上发生的 SOC 桶更新。

**见解 4：使用 FDP 的数据放置可以帮助 CacheLib 控制 DLWA**。CacheLib 可以利用 FDP 使用不同的回收单元句柄在 SSD 中分离 SOC 和 LOC 数据。这允许 LOC 数据和 SOC 数据驻留在互斥的 SSD 块(回收单元)中。这样的设计将具有以下好处：

1. 包含 LOC 数据的 SSD 块被顺序覆写，导致最小的数据移动和 DLWA(图 3b 2b)。如果 LOC 数据驻留在与 SOC 数据不同的回收单元中，设备过度配置空间可以专门由 SOC 数据使用。
2. SOC 数据仅使自身无效的理想行为(见解 3)可以通过将其隔离到单独的回收单元中实现(图 3b 2a)。较小的 SOC 大小导致更高的无效率，使得 SSD 擦除块中的大部分 SOC 数据无效。这导致最小的实时数据移动和 DLWA。随着 SOC 大小的增加，即使在回收单元之间隔离 LOC 和 SOC，我们也预计 DLWA 会增加。
3. 使用 FDP 可以实现设备过度配置空间的理想利用(见解 2)(图 3b 2c)。SOC 数据可以使用过度配置空间来缓解 DLWA。当 SOC 大小小于设备过度配置空间时，我们预计 DLWA 约为 1，因为每个 SOC 数据块至少有一个备用块可用。
4. SSD 中 LOC 和 SOC 数据的分离不需要更改 CacheLib 架构和 API。因此，我们预计应用程序级写入放大(ALWA)不会发生变化。

**见解 5：初始隔离 FDP 设备足以控制 CacheLib 中的 DLWA**。随着 SSD 中 LOC 和 SOC 数据的分离，唯一的实时数据移动将是由于 SOC 数据。无论 SSD 是初始隔离还是持久隔离，用于垃圾回收的回收单元中只会驻留 SOC 数据。因此，无论如何都将保留 LOC 和 SOC 数据的隔离。

**见解 6：使用 FDP 的数据放置可以帮助减少 CacheLib 中的碳排放**。与运营碳排放相比，内含碳排放占碳排放的主要部分。使用 FDP 启用的 CacheLib 的 DLWA 收益导致设备寿命改善。这导致系统生命周期内设备更换次数减少，从而减少内含碳排放。

较少的垃圾回收操作是 FDP 启用的 CacheLib 获得 DLWA 收益的原因。对于固定数量的主机操作，较少的数据迁移导致较少的总设备操作。总操作的减少要求设备在活动状态下花费较少的周期，导致 SSD 能源消耗降低和运营碳足迹减少。

### 4.2 FDP 启用的 CacheLib DLWA 和碳排放的理论分析

我们使用上一节的见解，为使用 FDP 的 CacheLib 中 SOC 和 LOC 数据隔离制定了 DLWA 和碳排放的理论模型。我们假设 LOC 数据的 DLWA 约为 1。此外，我们使用的事实是，只有 SOC 数据将使用设备过度配置空间，并且对 SOC 桶的项目插入遵循统一哈希函数。为了简化我们的分析，我们假设 CacheLib 中使用的统一哈希函数表现良好。

**定理 1**：使用 SOC 和 LOC 数据隔离的 FDP 启用的 CacheLib 的 DLWA 为：

$$
DLWA = \frac{1}{1 - \delta}
$$

其中 δ 表示由于垃圾回收导致的平均实时 SOC 桶迁移，由以下公式给出：

$$
\delta = -\frac{S_{SOC}}{S_{P - SOC}}\times \mathcal{W}(-\frac{S_{P - SOC}}{S_{SOC}}\times e^{-\frac{S_{P - SOC}}{S_{SOC}}})
$$

其中 S_SOC 是总 SOC 大小(以字节为单位)，S_P-SOC 是包括设备过度配置在内的 SOC 数据可用的总物理空间(以字节为单位)，W 表示 Lambert W 函数。

**4.2.1 为 FDP 启用的 CacheLib 建模 CO2 排放(CO2e)**：总碳足迹是内含和运营碳排放的总和。

$$
\mathrm{Total}_{\mathrm{CO2e}} = C_{\mathrm{embodied}} + C_{\mathrm{opertional}}
$$

**定理 2**：通过考虑系统生命周期 T 年和额定 SSD 保修 L_dev 年期间的 SSD 更换，使用 CacheLib 产生的内含碳排放为：

$$
C_{embodied} = DLWA\times Device_{cap}\times \frac{T}{L_{dev}}\times C_{SSD}
$$

其中 Device_cap 是设备的物理容量。Host_cap = Device_cap × (1 - Total_op)表示主机系统在 G_op 中使用的 SSD 容量，D_op∈[0,1)是主机过度配置和设备过度配置的分数，C_SSD 是每 GB 制造的 SSD 产生的 CO2e(Kg)量。

运营能源可以使用温室等效计算器转换为 CO2 排放(CO2e)。可以通过估计在空闲和活动状态下花费的时间来建模消耗的运营能源。在活动状态下花费的时间与相关期间内的总设备操作数成正比。

**定理 3**：运营能源与垃圾回收事件的总数成正比。

$$
\mathcal{E}_{\mathrm{opertional}}\propto \mathcal{E}(Hot_{\mathrm{operations}}) + \mathcal{E}(Dev_{\mathrm{migrations}})
$$

其中，Dev_migrations 是 SSD 中触发的垃圾回收操作的数量。

## 5 设计与实现

在本节中，我们概述了在 CacheLib 中构建 FDP 感知 SOC 和 LOC 数据隔离的设计原则、实现细节和经验教训。

### 5.1 设计原则

CacheLib 是各种缓存服务的流行构建块，具有多样化的用例。重要的是设计一个最小侵入性、适应性强且可维护的 SOC 和 LOC 数据隔离解决方案。我们在构建 CacheLib 中基于 FDP 的数据隔离支持时应用了以下设计原则：

1. **保持简单**：这一指导原则使我们能够将我们的更改合并到上游，考虑到 CacheLib 的多样化用例以及代码库的稳定性和可维护性需求。

2. **FDP 只是 CacheLib 的另一种存储技术**：设计应无缝支持不使用 FDP 的部署和设置，只需最小的配置更改即可保持用户友好性。这确保 CacheLib 对不使用 FDP SSD 的用户保持向后兼容性。

3. **允许各种数据放置技术的软件可扩展性**：设计应该是通用和可扩展的，允许 CacheLib 中现有和未来的模块隔离数据并尝试各种数据放置策略和决策。

4. **允许不断发展的数据放置技术的硬件可扩展性**：由于 FDP 规范是新的，预计会随着时间的推移而发展，其使用应该在 CacheLib 架构中本地化，以最小化随着硬件技术的发展对代码库的更改。

![CacheLib I/O路径](d:\ReadingPaper\Paper_in_md\Towards Efficient Flash Caches with Emerging NVMe.pdf-bde1b412-5210-41ef-8477-aaf2a7d18544\images\835269838dc4dd5502ee2ed69e994e353e82a484371cd37d4d3ce73a51ae4962.jpg)

图 4. CacheLib I/O 路径。1a 表示放置句柄分配器，负责分配消耗 FDP 的放置句柄。

### 5.2 放置句柄

我们在 CacheLib 的 SSD I/O 路径中引入了放置句柄的抽象概念，以实现基于 FDP 的数据放置，同时保持向后兼容性。放置句柄允许各种消费模块隔离数据。可用的放置句柄集在初始化时从数据放置感知设备层分配。这种抽象隐藏了底层数据放置技术(例如，FDP)的语义，提供硬件可扩展性。如果底层设备未启用 FDP，则使用默认放置句柄来表示没有放置偏好。

### 5.3 放置句柄分配器

我们实现了一个放置句柄分配器(图 4 1a)，负责为任何希望使用数据放置的模块分配放置句柄。如果在 CacheLib 中启用了 FDP，并且底层 SSD 支持 FDP，则此模块将可用的<RUH, RG>对(在 FDP 规范中称为放置标识符(PID))分配给它分配的放置句柄。这从 FDP 的消费者中抽象出 FDP 语义。如果底层 SSD 不支持 FDP，将分配默认句柄。这表示没有放置偏好。在 CacheLib 中，每个 I/O 引擎对中的 SOC 和 LOC 在初始化期间获得不同的放置句柄分配。SSD 的次要消费者模块(例如，元数据)不声明其放置偏好，因此为它们分配默认回收单元句柄。放置句柄分配器的引入提供了软件可扩展性和灵活性。

### 5.4 FDP 感知 I/O 管理

SOC 和 LOC 实例用唯一的放置句柄标记它们的 I/O。FDP 感知设备层将这些句柄转换为相应的 FDP 放置标识符(PID)。PID 进一步转换为 NVMe 规范放置指令字段(DSPEC 和 DTYPE 字段)，附加到 uring_cmd I/O，然后提交。我们使用 I/O Passthru 机制将启用 FDP 的 I/O 发送到 Linux 内核。我们为每个工作线程使用一个 io_uring 队列对，以便可以将 I/O 发送到内核，而不会在 io_uring 的提交和完成队列中出现同步或并发挑战。FDP 感知 I/O 管理层通过抽象启用 FDP 的 SSD 的布局提供硬件可扩展性。

### 5.5 经验教训和未来方向

在本节中，我们概述了随着时间的推移，在 CacheLib 中集成 FDP 功能时学到的一些经验教训。其中一些经验可能对未来的工作方向有用。

1. **CacheLib 中 FDP 专用 LOC 驱逐策略没有提供太多好处**：我们通过在 LOC 的基于区域的驱逐策略中构建回收单元大小感知，探索了 CacheLib 的专用 FDP 驱逐策略的概念。FDP 规范提供了主机跟踪对回收单元的写入所需的语义。利用它，我们可以跟踪属于回收单元的 LOC 区域，并一起使多个区域在回收单元中无效。这可以与 TRIP 命令配对，以释放整个回收单元并帮助 SSD 上的垃圾回收过程。对这种策略的早期探索显示收益最小，因此被搁置。我们推测，在回收单元较小的情况下，这种策略可能会有益。

2. **动态和自适应数据放置的性能不如简单的静态解决方案**：使用 FDP 事件日志，主机可以了解 SSD 中的垃圾回收操作。这允许主机软件实时了解其数据放置决策的影响。主机软件堆栈可以利用日志构建反馈循环，以了解其放置决策并相应地调整。我们在项目早期使用各种负载平衡和数据温度技术探索了一些动态数据放置策略。然而，与为小对象主导的混合工作负载隔离 SOC 和 LOC 数据的静态预定义放置句柄相比，我们看到的收益与这种努力的工程复杂性相比微不足道。

## 6 评估

在本节中，我们探讨在 FDP SSD 上隔离 CacheLib 的小对象缓存和大对象缓存中的数据的好处和局限性。

### 6.1 实验设置

本节描述系统设置、实验的工作负载和评估中使用的指标。

**硬件和软件设置**：我们在实验设置中使用两台具有类似硬件特性的不同机器。两台机器都有两个 24 核 Intel Xeon Gold 6432 处理器，约 528 GB 的 DRAM。每台机器使用一个 1.88 TB 的三星 PM9D3 SSD，固件版本支持 FDP 规范。设备上的 FDP 配置支持 2 个命名空间、1 个 RG 和 8 个可以同时使用的初始隔离 RU 句柄。对于所有实验，我们创建一个单一命名空间并将所有 RU 句柄映射到它。每个 RU 的大小约为 6 GB。其中一台机器运行 Ubuntu 22.04，Linux 内核为 6.1.64，而另一台机器运行 CentOS 9，Linux 内核为 6.1.53。我们使用 nvme-cli 版本 2.7.1 配置 SSD 上的 FDP 功能。

**系统比较**：对于我们的实验，我们使用 CacheLib 存储库的主分支，其中包含我们上游的基于 FDP 的数据放置更改，以隔离 SOC 和 LOC。我们使用 nvme-cli 在控制器上启用和禁用 FDP 功能，以对比传统 SSD 和 FDP SSD。在本文的其余部分，我们使用以下术语来突出测试中的系统比较：

1. **FDP**：在 CacheLib 中启用基于 FDP 的数据隔离，并在 SSD 上启用 FDP 配置。
2. **非 FDP**：在 CacheLib 中禁用基于 FDP 的数据隔离，并在 SSD 上禁用 FDP 配置。

我们使用 CacheBench 工作负载生成器和跟踪重放工具来运行工作负载。CacheBench 是一个应用程序，在同一进程中调用 CacheLib 缓存 API，可用于运行捕获的跟踪或生成基准测试。所有用于运行实验的脚本都可以公开获取。

**指标**：我们专注于 DLWA 作为评估我们的数据隔离更改有效性的主要指标，因为它与耐久性和内含碳排放直接相关。我们使用 nvme-cli 工具查询来自 SSD 控制器的日志页面(nvm get-log)，该控制器跟踪主机写入它的字节和在 10 分钟间隔内在 NAND 媒体上写入的设备字节。我们在每次实验前通过对整个设备大小发出 TRIM 来将 SSD 重置为干净状态。我们使用 CacheBench 工具测量和报告我们实验的吞吐量、延迟、DRAM 和 NVM 缓存命中率以及 ALWA。

**工作负载**：我们的实验使用来自 Meta 的键值缓存(KV Cache)集群的采样 5 天匿名跟踪和来自 Twitter 的 cluster12 的 7 天匿名跟踪，这些跟踪可公开用于研究和实验。KV Cache 是一个读密集型工作负载，其中 GET 操作比 SET 操作多 4:1。对于 KV Cache，我们使用约 42 GB 的 DRAM 和约 930 GB 的 SSD(50%设备利用率)作为默认设置进行缓存。Twitter 的 cluster12 工作负载是写密集型的，其中 SET 操作比 GET 操作多 4:1。对于 Twitter 工作负载，我们使用约 16 GB 的 DRAM 和约 930 GB 的 SSD(50%设备利用率)作为默认设置。为了在更短的时间内更多地压力测试 SSD 并生成高 DLWA 场景，我们通过从 KV 缓存跟踪中删除 GET 操作生成了一个额外的仅写 KV 缓存工作负载，使其几乎完全由 SET 操作组成。我们将其称为 WO KV Cache 工作负载，并使用与 KV Cache 相同的 DRAM 和 SSD 配置。

### 6.2 基于 FDP 的隔离实现了约为 1 的 DLWA

我们使用默认 DRAM 缓存大小约 42 GB 和 SSD 缓存大小约 930 GB(总大小为 1.88 TB SSD 的 50%有效利用率)运行 KV Cache 工作负载。默认配置中的 SOC 大小设置为 SSD 大小的 4%。图 5 显示了超过 2 天运行持续时间的间隔 DLWA。我们可以看到，将 SOC 和 LOC 数据隔离到两个不同的回收单元句柄有助于将 DLWA 从没有数据隔离时观察到的 1.3 降低到 1.03。这验证了我们的分析，即通过利用设备过度配置，将 LOC 的顺序和冷写入模式与 SOC 的随机和热写入模式隔离有助于控制 DLWA。因此，我们通过 CacheLib 中基于 FDP 的隔离实现了理想的约为 1 的 DLWA。

![DLWA随时间变化](d:\ReadingPaper\Paper_in_md\Towards Efficient Flash Caches with Emerging NVMe.pdf-bde1b412-5210-41ef-8477-aaf2a7d18544\images\3bfeeca04cf21bea587c8f935226d4157d4351bcc961c8ca1a52f00aa7f1bdf2.jpg)

图 5. 使用 KV Cache 工作负载，50%设备利用率，42GB RAM 和 4% SOC 大小，60 小时内的 DLWA。基于 FDP 的隔离导致 DLWA 减少 1.3 倍。

### 6.3 基于 FDP 的隔离使得更好的 SSD 利用率成为可能，而不影响性能

图 6 显示了改变用于缓存的 SSD 容量对 CacheLib 中各种重要指标的影响。这个实验量化了增加 SSD 用于缓存的主机级利用率的效果。我们看到，当不采用数据隔离时，DLWA 从 50%利用率时的 1.3 增加到 100%利用率时的 3.5，但当采用数据隔离时，DLWA 在各种利用率下保持不变，约为 1.03。为简洁起见，我们省略了 50%和 90%之间利用率的数据点，因为它们与 50%类似。这个结果验证了我们对设备过度配置和 SOC 无效率是影响 DLWA 的关键因素的分析。在 4% SOC 大小下，SOC 桶的冲突率非常高。这导致 SSD 中 SOC 数据的高无效率。由于 SOC 数据与 LOC 数据隔离，这种高无效率将导致由于垃圾回收而导致的最小有效数据移动。此外，SSD 设备过度配置容量(占 SSD 容量的 7-20%)现在可以专门由 SOC 数据使用，以减少垃圾回收的影响。由于 SOC 的随机写入仅占 SSD 容量的 4%，即使在 100%设备利用率下，它们的影响也被 SSD 保留的额外空闲块所吸收。我们可以看到，与基线相比，使用基于 FDP 的隔离，吞吐量、DRAM 和 NVM 缓存命中率指标保持不变。随着利用率的增加，p99 读取和写入延迟也由于垃圾回收过程的干扰减少而显示改善。在 100%设备利用率下，p99 读取和写入延迟分别显示 1.75 倍和 10 倍的改善。由于我们没有更改数据在 SOC 和 LOC 中的存储方式，我们预计 ALWA 不会发生变化，这一点我们从 CacheBench 日志中确认了。
